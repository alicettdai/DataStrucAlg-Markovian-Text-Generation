Name: [alice dai]
NetID: [ad322]
Hours Spent: [15.5]
Consulted With: [Emily Liu, Michael Casio, Azim Dharani]
Resources Used: [A LOT of stackexchange and googling java APIs]
Impressions: [This was very time consuming, and a lot of work to be completely honest. Not because the work itself was difficult, but because I was trying
to do things with a language I'm not really familiar with. But I learned a ton, and I decided that I'd try to do this project relatively independently, and 
learn how to google things and ask the right questions to the Internet before talking with my friends. Probably the most useful part of this assignment was
feeling like I could do it on my own and get myself out of my confusion.]
%%%%
PROBLEM 1:
For alice.text-
k	#chars	source	mean  	sigma
1	2000		163187	0.823	0.031
1	4000		163187	1.475	0.000
1	8000		163187	2.993	0.001
1	16000	163187	5.936	0.003
5	2000		163187	0.511	0.000
5	4000		163187	1.052	0.000
5	8000		163187	2.087	0.000
5	16000	163187	4.209	0.003
10	2000		163187	0.533	0.001
10	4000		163187	1.039	0.000
10	8000		163187	2.082	0.000
10	16000	163187	4.169	0.001

Varying file length, using k=5 and text length 400

unique	size			name				#chars	mean		sigma
8122		13107		poe.txt			400		0.009	0.000
31012	82140		melville.txt		400		0.055	0.000
42634	153088		romeo.txt		400		0.109	0.000
42016	163189		alice.txt		400		0.102	0.000
82337	496769		hawthorne.txt	400		0.319	0.000
180176	4345018		kjv10.txt		400		2.778	0.004

For hawthorne.text-
k	#chars	source	mean		sigma
1	2000		496768	2.672	0.015
1	4000		496768	5.151	0.024
1	8000		496768	10.558	0.375
1	16000	496768	19.851	0.042
5	2000		496768	1.611	0.001
5	4000		496768	3.344	0.004
5	8000		496768	6.755	0.018
5	16000	496768	13.668	0.195
10	2000		496768	1.654	0.000
10	4000		496768	3.295	0.001
10	8000		496768	6.724	0.015
10	16000	496768	13.467	0.044

Varying file length, using k=5 and text length 400

unique	size		name				#chars	mean		sigma
8122		13107	poe.txt			400		0.011	0.000
31012	82140	melville.txt		400		0.054	0.000
42634	153088	romeo.txt		400		0.117	0.000
42016	163189	alice.txt		400		0.106	0.000
82337	496769	hawthorne.txt	400		0.332	0.000
180176	4345018	kjv10.txt		400		2.890	0.007

%%%%
PROBLEM 2:
I put a timer to both alice.text and hawthorne.text test from question 1. alice.text ran for 5 minutes, and hawthorne.text ran for
15 minutes. I timed it with minutes just to get a better sense of the duration, since the mean doesn't give me a good sense of time. 
Anyway, I timed the run times for alice.txt and hawthorne.text, and also calculated that hawthorne is 3 times as long as alice, which 
would be a good indication that the relationship between text length and run time is linear, since hawthorne also took three times as
long to run compared to alice. Also, the means within a k also double as #chars double, and between k's the runtime are should get faster,
as is more or less the trend in both files (the run times in the beginning of k=5 are faster than that of k=10).  

Given these observations of linear trends, the mean time to generate 16,000 random characters for an order-5 Markov model 
with a training text of 5.5 million characters should be (5.5 million divided by 496768) 11 times longer than hawthorne's mean run
time with the same order and char #, so that's 13.668*11 = 151.

%%%%
PROBLEM 3:
For alice.text-
k	#chars	source	mean		sigma
1	2000		163187	0.029	0.001
1	4000		163187	0.015	0.000
1	8000		163187	0.013	0.000
1	16000	163187	0.019	0.000
5	2000		163187	0.026	0.000
5	4000		163187	0.024	0.000
5	8000		163187	0.057	0.001
5	16000	163187	0.030	0.000
10	2000		163187	0.035	0.000
10	4000		163187	0.033	0.000
10	8000		163187	0.050	0.000
10	16000	163187	0.054	0.000

Varying file length, using k=5 and text length 400

unique	size		name				#chars	mean		sigma
8122		13107	poe.txt			400		0.002	0.000
31012	82140	melville.txt		400		0.012	0.000
42634	153088	romeo.txt		400		0.021	0.000
42016	163189	alice.txt		400		0.023	0.000
82337	496769	hawthorne.txt	400		0.073	0.000
180176	4345018	kjv10.txt		400		1.052	0.162

For hawthorne.text-
Starting tests
k	#chars	source	mean		sigma
1	2000		496768	0.048	0.001
1	4000		496768	0.032	0.000
1	8000		496768	0.038	0.000
1	16000	496768	0.030	0.000
5	2000		496768	0.093	0.001
5	4000		496768	0.072	0.000
5	8000		496768	0.072	0.000
5	160000	496768	0.096	0.000
10	2000		496768	0.131	0.002
10	4000		496768	0.122	0.000
10	8000		496768	0.124	0.000
10	16000	496768	0.127	0.000

Varying file length, using k=5 and text length 400

unique	size		name				#chars	mean		sigma

8122		13107	poe.txt			400		0.002	0.000
31012	82140	melville.txt		400		0.012	0.000
42634	153088	romeo.txt		400		0.022	0.000
42016	163189	alice.txt		400		0.025	0.000
82337	496769	hawthorne.txt	400		0.080	0.000
180176	4345018	kjv10.txt		400		0.969	0.139

Both these runtimes (in minutes) took less than a minute to complete, and looking at the two tables for alice and hawthorne,
hawthorne takes about twice as long to run as alice, but hawthorne is three times the length of alice. However, in EfficientMarkov, 
the length of the text is way less indicative of the run time trends, since the text file is only run once. I also noticed that doubling
the char# doesn't double the runtime, it actually decreases in most instances, which is weird since you'd think that increasing
the char# would require more time. I think that the relationship between source size and run time is 3:2. If you triple the file size,
you're doubling the run time. Using proportions (3/2=11/x), I get that a file size of 5.5million, which is 11 times larger than Hawthorne should
take x=7.3 times as long to run, which is a significant improvement to 11 from question 2. This translates to roughly 0.096*7.3=0.708, which 
is 213 times faster than my prediction in question 2. 


%%%%
PROBLEM 4:
Output with testfile.txt-
5 markov model with 448 chars
----------------------------------
 and exclamation will do, but why won't it work? Because it's 
stupid. What's up with this? It's a silly thing, a really silly 
thing, a really silly thing. The reason that finding sentences 
works is because it's stupid. What's up with this? It's only 
a test. It's only a test. It's a silly thing, a really silly 
thing. The reason that finding sentences work? Because! Simply 
because of punctuation. Any punctuation points do a bang up job!! 
Get it? 
----------------------------------

On average, 448 characters will be generated.
%%%%
PROBLEM 5:
In general, hashmaps are more efficient than tree maps. Hashmaps aren't sorted, which saves time and computing power, and the hashcode 
system makes sure that all the keys in the map are in unique places. I made a timer code on WordMarkov drive just to test this theory for 
smaller files, and for HashMaps, the time elapsed in milliseconds is 93, and for TreeMaps, it was 107 milliseconds. This was a test for
the Trump speech. I also ran the driver using Hawthorne, which is much longer, and the runtime for hashmap was 470 ms, and the runtime
for treeMaps was 1365 ms. For larger files, it becomes even more apparent that HashMaps are more efficient. 
%%%%
